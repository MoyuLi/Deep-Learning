{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 21.207613\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 10.1%\n",
      "Minibatch loss at step 500: 2.951984\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 1.868224\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 1.396508\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 2000: 0.979377\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 1.000356\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3000: 0.733033\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 605.912964\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 35.4%\n",
      "Minibatch loss at step 500: 203.684296\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 1000: 116.510956\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 70.076828\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 41.508465\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2500: 25.274145\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 15.481795\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_steps = 101\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting with small batch size\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 478.033508\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 36.0%\n",
      "Minibatch loss at step 2: 1056.360229\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 40.3%\n",
      "Minibatch loss at step 4: 695.020081\n",
      "Minibatch accuracy: 39.1%\n",
      "Validation accuracy: 51.3%\n",
      "Minibatch loss at step 6: 195.730057\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 62.9%\n",
      "Minibatch loss at step 8: 78.224777\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 10: 30.366920\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 12: 10.466361\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 14: 13.940823\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 16: 0.350591\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 18: 0.000114\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 22: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 24: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.1%\n",
      "Test accuracy: 81.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = ((step % num_batches) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 470.429840\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 33.1%\n",
      "Minibatch loss at step 2: 742.905029\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 48.2%\n",
      "Minibatch loss at step 4: 84.379196\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 6: 8.989225\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 59.4%\n",
      "Minibatch loss at step 8: 27.732651\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 62.4%\n",
      "Minibatch loss at step 10: 7.853378\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 12: 2.304717\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 14: 0.235868\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 16: 0.539414\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 18: 0.190587\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 20: 2.655365\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 64.2%\n",
      "Minibatch loss at step 22: 0.712038\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 28: 0.791599\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 34: 0.618019\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.1%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 44: 0.410540\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 48: 0.293477\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 54: 0.005928\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 56: 0.679071\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 60: 2.005158\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 68: 0.658218\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 78: 0.098512\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 82: 0.335621\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 88: 2.919093\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 98: 0.374949\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Test accuracy: 75.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.368801\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 33.7%\n",
      "Minibatch loss at step 500: 1.042260\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1000: 0.875479\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 1500: 0.813061\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 2000: 0.544452\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2500: 0.558416\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3000: 0.564924\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 3500: 0.498947\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4000: 0.528143\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Test accuracy: 94.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
